Log of Updates and Files

2/10/21 - evening
Trying to use astropy to create cutouts (which should be super easy) and struggling... The center pixel indices are in the thousands? It's 101x101. Makes no sense. Totally works if I just put in (30, 54) or something.
I also tried to open the weight map to see if it was just some odd mistake from the LinKS header. I couldn't even access the data from the primary hdu. "error - buffer is too small for requested array"

2/10/21 - afternoon
Response from J Nightingale (louisville.edu account) super helpful. The weight coadd is what I need. He gave me two functions that will produce the noise map from the weight map (or inverse noise map). The difference between the two is a squared term.
I had the wrong tile for the candidate I'm looking at. Have to go to a meeting with UCLA grad.

2/10/21 - morning
Response from K. Kuijken quite helpful. I was stupid, it's actually DR4, but it should be exactly the same anyway.
Note: pixel scale is 0.2 (I calculated 0.198 so that's fine)
Exposure times: g - 900s , r - 1800s, i - 1200s
Wrote functions to convert to counts.
Combined counts header info extraction, conversion, resizing, psf generation into one function (one_ring_to_rule_them_all)
Need to include the noise map when I figure that out. Breaking for lunch.


2/9/21
Still no response from Benne. I need to figure out this noise map thing. The FITS header is useless. I *think* they are in counts because the gain factor is huge and the signal is tiny, so I think it has already been converted to counts (ADUs). Maybe I completely misunderstood that, but hey how am I supposed to make something out of nothing. The lack on information in the images is frustrating. Met with Benne, and he worked through a lot of it with me. See that log for our discussion. Emailed Konrad Kuijken about DR3.

2/5/21
Tried taking sqrt of pixel values. Did not work at all.
Emailed James Nightingale (autolens creator), who said I'd need to know units of data (e-/s, counts), exposure time to convert to counts, and sky background level if not already subtracted.

2/4/21
The noise map in tutorials shows features from the galaxies... Perhaps I need to rethink that procedure. Here's how the API for autolens describes it: 'An array describing the RMS standard deviation error in each pixel in units of electrons per second.' I think that's what I've done? What about these units e-/s? Image is also that.
Downloaded tile and catalog tables that should give each tile's FWHM of psf.
To ask Benne: Gain - electrons per second?
Psf convolution was struggling, which I believe is because I made the image too small and the psf too large. Reworked some prep. Ran into another config file problem when I tried to run the model... which had inf loglikelihood measurements. Something is completely wrong that I need to figure  out.. I'm wondering if we misinterpreted the noise map generation. Perhaps I need to add the image back onto it? Why would that be the case?
Summary:
- Have tile PSFs for g, r, i
- Model starts to run; problem appears to be in noise map - don't think I have what it needs

**** I decided on 2/4/21 that it makes more sense to update at the top... ****
1/14/21
Reviewing all the work prior to today.
Cut the samples acc. to (z_lens > 0.05) and (z_src - z_lens > 0.1)
Created directory w/in csv files labeled "latest"
Most recent data as of beginning of session:
    links_autoz_sample_latest.csv (links_autoz_sample_061520) (56 rows, 52 unique candidates)
    links_knabel_autoz_sample_latest.csv (...061520) (7 rows, 6 unique candidates)
    li_autoz_sample_latest.csv (...061520) (8 rows, 8 unique candidates)
Most recent data following end of session:
    links_sample_latest_len42.csv (42 rows, 40 unique candidates)
    links_knabel_sample_len7.csv (7 rows, 6 unique candidates)
    li_sample_len3.csv (3 unique candidates)
Notes:
    I have not determined how I will choose one of the duplicates over the other.

1/15/21
Working with final two duplicates in LinKS candidates. Simple to cut.
Most recent data following session:
	links_sample_latest_len40
	links_knabel_sample_len7
	li_sample_len3
Notes:
	I intend to combine the notebooks and visualizations and begin writing.   
    
1/15/21
Consolidated work to a master notebook. Got through the most recent selection.
Most recent data following session:
	links_sample_latest_len40
	links_knabel_sample_latest_len6
	li_sample_latest_len3
Next step to pull the visualization pieces into the master notebook.
Notes:
    Check those data against the ones saved in the latest folder earlier today.
        All candidates should be the same... hopefully I got the Lambdar stuff 
        right the first time

1/18/21
Consolidated visualization code to master notebok.
Looked at two LinKS candidates that appear to fall within the selection parameter space used by Holwerda-15... One of them passes, and I have no idea why it wasn't selected in the paper.
It could have been on an alias... All candidates, old and new, near the alias of
(1+z)/(1+z2)=1.343±0.002 (∼5007/3727) or the inversewere removed from the sample.
For this candidate, the result is 1.344, which shows that it was removed by happenstance. With a log(mass) ~ 11.2 and redshift 0.22, it falls right at the overlap between spec and mac in the Knabel-2020 paper. Interesting!
Discuss with Benne the relevant info to focus on.

1/19/21
Notebook: 011921_correlation_tests
Applying tests of correlation for output parameters from autoz and lens scores.
In master notebook, CNN prob is incorrectly merged. Change that!
Ran spearman, pearson and kendall tau tests on the scores and cnn probability output to sigma2 and R. Scores were not well-correlated at all. CNN probability output was ~0.25, but there are a couple outliers. It may be useful to bootstrap and check again.

1/20/21
Notebook: 011021_correlation_tests
Fitted linear regression to the parameters. 
Very weak correlations, but I at least have some numbers to it. 
Ran some bootstrapping tests to see about uncertainties and get more info on how outliers affect it... Not sure how to do all of that properly. Emailed Benne. I used the result of the fit and put the uncertainty as +/- the std from the bootstrapping.
Added these results to the paper.
Still need to fix CNN prob merging in master notebook and put correlation studies in the master as well.

1/21/21
Updated master notebook with the correlation studies and visualizations.
Also removed a LinKS candidate whose probability of redshift success in autoz was very low.
This forced me to redo the correlation studies, which changed the numbers slightly. No change in outcome.
Looked at the z-lens redshift outputs for Li candidates against Li photo-z, which are inconsistent for two of the three. Will need to make a decision there. I need to check the z-src as well.

1/21/21
Checking on duplicates I came upon the concerning fact that I had mixed up ELG + ELG and PG + PG. Has minor consequences, but I'm glad I caught it here.
In response to my concern about redshift matches having sigma_source>sigma_lens, Benne:
    "That is more a strength of the emission line rather than anything else. This is a flux-weighted result after all. 
So if you have a whopping Halpha line, it's going to be the primary solution, even though the continuum is a lower redshift. SO this is fine.""
To Do:
Overlay the redshift and stellar mass results over the results from Knabel-2020
Further explore primary redshift as background source
Flesh out writing sections on "what could have been" and results from Li.

1/22/21 - Morning
Notebook: 012221_autoz_samples_comparison_to_knabel2020 (copied to master notebook)
Plotted lens redshift to lens stellar mass on top of big plot from knabel2020.
    All autoz candidates are at log(m*) > 11.0 but at the lower end of the range of LinKS-Knabel2020 candidates, throughout redshift range.
Calculated PM and SIS Einstein radius estimates using AUTOZ source redshifts and comparing to the 2:1 ratio we used in Knabel-2020.
    In general, AUTOZ source redshifts gave lower estimates compared to 2:1 ratio, which have mean ~ or < 1.0 arcsec.
    It appears they are typically on the lower end of the machine learning candidate mass range and Einstein radius. Especially looking at the LinKS sample, it appears these candidates have estimated Einstein radii of ~1 arcsec or less, which fits our idea that they would have a better shot of being detected by AUTOZ. Most of these didn't make the cut based on the visual inspection scores given by Petrillo, which also makes sense if their Einstein radii are particularly small (leading to low scores).
    (mean, median, std)
    LinKS PM:  1.138 0.984 0.535
    LinKS PM 2to1:  1.241 1.171 0.414
    Average Difference:  0.102
    --
    LinKS SIS:  0.857 0.771 0.352
    LinKS SIS 2to1:  1.042 1.015 0.244
    Average Difference:  0.185
    --
    LinKS Knabel-2020 PM:  1.033 0.918 0.192
    LinKS Knabel-2020 PM 2to1:  1.248 1.199 0.317
    Average Difference:  0.214
    --
    LinKS Knabel-2020 SIS:  0.757 0.726 0.141
    LinKS Knabel-2020 SIS 2to1:  1.094 1.181 0.263
    Average Difference:  0.336
    --
    Li PM:  1.304 1.043 0.777
    Li PM 2to1:  1.26 1.257 0.528
    Average Difference:  -0.044
    --
    Li SIS:  0.98 0.819 0.475
    Li SIS 2to1:  0.965 1.023 0.258
    Average Difference:  -0.015
I want to show this difference a little better and discuss all of this in the paper. For this afternoon.

1/22/21 Afternoon
I looked at mean and median stellar masses and Einstein radius estimates. KS tests show disparity to high significance between stellar mass of LinKS AUTOZ and LinKS from Knabel-2020, and an almost identical result when compared to the GAMA spectroscopic sample.
LinKS - Mass:  [0.352 0.007] (metric pvalue)
LinKS/Spec - Mass:  [0.353 0.007]
Also looked at G544226, which would have been the overlap between GAMA spec and LinKS in Knabel2020. It is the ideal for identification by both.
Plan to rename Li candidates to BG (bright galaxies) or Li-BG for sake of distinguishing them from LinKS.
Still more to write in the paper, and I need to start on reviewing SVMs on Monday.

1/25/21
ML for classification - SVMs (SVC), K-NearestNeighbor, supervised vs unsupervised...
    What is my test/train?
    KNN : curse of dimensionality
    Cross-validation strategies: KFold, etc. https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html
Stopped for lunch at sklearn tutorial 3 - unsupervised learning
Unsupervised learning:
    K-means clustering (sparsity mitigates curse of dimensionality)
    Hierarchical - Agglomerative - bottom-up approaches
        feature agglomeration mitigates curse of dimensionality
    Transforms, Decomposition, PCA to reduce dimensionality
        ICA
Pipelines... Maximize cross-validation scores from multiple algorithms... e.g. PCA and SVC?
Quadratic Surface SVM? - SVM are best for supervised learning. Adapt to unsupervised?
    https://www.sciencedirect.com/science/article/abs/pii/S0377221719306630

1/26/21
Writing more of the paper.
Reviewing ML...
Starting to think I'll want to begin with SVM supervised, tell it what to do. See what it says about the sigma2 and R parameter space.
Then give it everything, let it decide with PCA what to determine, still keep it supervised.
Then try it k-means clustering and PCA and see if it marks off anything similar.

1/28/21
Benne wanted to scrap the ML idea for now... I'll come back to it.
We're going to focus on the lens modeling, which is more fun anway!
I've reviewed PyAutolens HowTos thru Ch2.5.
Tomorrow:
Finish 6-9, look at preprocessing
Take a look at some real images
Hopefully model a KiDS image next week.

1/29/21
Finished autolens tutorials, prepping G3575500 (1906) for modeling.
Need to figure out the psf stuff and whether I should stack the images... I think that's what I did before.

2/3/21
Had a bunch of issues trying to make autolens do anything that isn't part of the tutorials.

