Log of Updates and Files

2/22/21 - morning
Following email response from Jammy, I am going to revise some of my model. This ensures we don't fit a local max.
*** Model
Phase 1 - Dynesty(n_live=40, tolerance=1, walks=default)
Phase 2 - Dynesty(n_live=120, tolerance=0.5, walks=10, facc=0.4)
***
I'm also trashing the pyswarms idea, on Jammy's suggestion.
Here's the final experiment for the first model.
6. Set positions(position_threshold=2.0)
    Set positions(position_threshold=1.5)
    Set positions(position_threshold=1.0)
    Set positions(position_threshold=0.5)
Then I will try it with the DM profile.
#
The positions can be set with a GUI or manually... Neither is working. Having Lutz update the autolens library in pacer.
I'm trying to set the first phase to give me the light profile and then use those to inform the priors for the lmp in the second phase, but it is not working because the lmp elliptical_comps are not working. I'll wait and see if the autolens library gets updated.

2/19/21 - morning
Experimenting with model fits for 1906 to see how different changes affect convergence time and log likelihood.
*** Best Model as of the end of today.
Phase 1 - Dynesty(n_live=20, tolerance=2, walks=default)
Phase 2 - Dynesty(n_live=40, tolerance=0.5, walks=5, facc=0.4)
***
Experiments
1. Adjusting Dynesty settings
    - n_live_points - # models being tested
    - evidence_tolerance - algorithm stops sampling when it estimates that continuing will not increase Bayesian evidence (log_evidence) beyond the tolerance (<0.8 for reliable error est); keep high for first phase, low for second
    - walks - # steps taken in param space by each live point (5-10 is optimal)
    - facc - how big each step is (0.2-0.3 optimal)
2. Different optimizers:
    - Particle Swarm Optimizer (PySwarm)
        iters - # steps for ea. live point (particle)
        Typically follow this model:
    1. Initialize with Dynesty
    2. Refine with PySwarm
    3. Finish with Dynesty
3. Set positions of lens images
    - Positions - identify positions which the algorithm first checks if they trace back within a certain arcsec threshold
    - Place on images... If they don't plot back to the same source, then the algorithm doesn't search the model
    - As long as you keep the threshold above ~0.5" you'll be fine.
EXPERIMENT: (First phase optimize speed, second optimize likelihood)
1. -1 Dynesty(n_live=60, all others unchanged), Dynesty(n_live=80, all other unchanged)
    -2 Dynesty(n_live=80, all others unchanged), Dynesty(n_live=80, all other unchanged)
    -3 Dynesty(n_live=60, all others unchanged), Dynesty(n_live=100, all other unchanged)
    -4 Dynesty(n_live=40, all others unchanged), Dynesty(n_live=60, all other unchanged) # The lower n_live_points is fast and just as good
    -5 Dynesty(n_live=20, all others unchanged), Dynesty(n_live=40, all other unchanged) *** Works fine and is fast! ***
    -6 Dynesty(n_live=10, all others unchanged), Dynesty(n_live=20, all other unchanged) 
    -7 Dynesty(n_live=5, all others unchanged), Dynesty(n_live=10, all other unchanged) # here is where things started to fail
  Experiment  phase1_time  phase1_time  phase2_time  phase2_likelihood  \
0        1-1   107.555079   296.239404   167.684566         118.462835   
0        1-2   137.521799   296.010310   143.622440         118.254286   
0        1-3   104.364316   296.235893   206.749381         118.070031   
0        1-4    73.453015   296.132388   132.761242         117.998034   
0        1-5    40.029995   296.179939    98.284560         118.632109   *** n_live = 20, 40 is best.  
0        1-6    34.548316   286.543298    65.271374         118.514028   
0        1-7    11.370712  -208.230643    22.767801          89.789176
---- Take best ---- 1-5
2. -1 Dynesty(n_live=best, evidence_tolerance=5), Dynesty(n_live=best, evidence_tolerance=0.8)
    -2 Dynesty(n_live=best, evidence_tolerance=10), Dynesty(n_live=best, evidence_tolerance=0.8)
    -3 Dynesty(n_live=best, evidence_tolerance=2), Dynesty(n_live=best, evidence_tolerance=0.8)
    -4 Dynesty(n_live=best, evidence_tolerance=2), Dynesty(n_live=best, evidence_tolerance=0.5) *** This is best ***
    -5 Dynesty(n_live=best, evidence_tolerance=2), Dynesty(n_live=60, evidence_tolerance=0.5) # trying higher n_live to see if difference... nope
---- Take best ---- 2-4
3. -1 Dynesty(n_live, tolerance=best), Dynesty(n_live, tolerance=best, walks=5) *** This is best ***
    -2 Dynesty(n_live, tolerance=best), Dynesty(n_live, tolerance=best, walks=10)
    -3 Dynesty(n_live, tolerance=best), Dynesty(n_live, tolerance=best, walks=20)
    -4 Dynesty(n_live, tolerance=best), Dynesty(n_live, tolerance=best, walks=50)
    -5 Dynesty(n_live, tolerance=best, walks=5), Dynesty(n_live, tolerance=best, walks=5)
---- Take best ---- 3-1
4. -1 Dynesty(n_live, tolerance, walks=best), Dynesty(n_live, tolerance, walks=best, facc=0.2)
    -2 Dynesty(n_live, tolerance, walks=best), Dynesty(n_live, tolerance, walks=best, facc=0.3)
    -3 Dynesty(n_live, tolerance, walks=best), Dynesty(n_live, tolerance, walks=best, facc=0.4) *** This is best ***
    -4 Dynesty(n_live, tolerance, walks=best), Dynesty(n_live, tolerance, walks=best, facc=0.5)
---- Take best ---- 4-3
*** Best Model so far:
Phase 1 - Dynesty(n_live=20, tolerance=2, walks=default)
Phase 2 - Dynesty(n_live=40, tolerance=0.5, walks=5, facc=0.4)
***
# Stopped here. Pick it up on Monday
5. Dynesty(fastest), PySwarm(n_particles=50, iters=1000), Dynesty(best) # These aren't working right now.
    Dynesty(fastest), PySwarm(n_particles=80, iters=1000), Dynesty(best)
    Dynesty(fastest), PySwarm(n_particles=50, iters=1500), Dynesty(best)
    Dynesty(fastest), PySwarm(n_particles=80, iters=1500), Dynesty(best)
---- Take best ----
6. Set positions(position_threshold=2.0)
    Set positions(position_threshold=1.5)
    Set positions(position_threshold=1.0)
    Set positions(position_threshold=0.5)
NOTE: First set the mask subsize to 2 (instead of 1)
    - r-band took 2 minutes as opposed to 3 with the subsize at 1.
    - g-band faster by 10 seconds.
    - STICK WITH 2!

2/18/21 - afternoon
Reran the fits and changed some small pieces according to Jammy's suggestions, which were:
1. Put tighter prior on r-band fit lens galaxy bulge center (fixed to 0.0, 0.0 or uniform priors with width 1/2 pixel [I chose the latter])
2. Directly link center of bulge and SIE mass.
3. Bulge intensities from r- to g-band lens... make sure 0.1 Gaussian Prior is near the solution. (It's ~0.5-0.6)
They turned out to produce essentially exactly the same max log likelihood.
I want to keep tweaking this, and I need to find the best way to extract dark matter fractions from this.

2/18/21 - morning
The fit appears to be converging! There was an error that it can't find the right config file for 'dataset', which is in the plots.ini from config/visualize. Updated the autolens_workspace from github, so it should all be right. The config files are in the working directory. In contact with J Nightingale right now.
----
I have my first models. I first fit the r-band image to get the light profile of the lens galaxy. I then fit the g-band image using the output of the first phase as priors for the second. I'm sending this data to Jammy (J Nightingale) to see if he has some thoughts for it.
I need to figure out exactly how I'm going to extract dark matter fractions here... M/L ratio? Einstein radius? I have stellar mass, so if I can just get a measure of the lensing mass, then I have the dark matter.

2/17/21 - morning
Reinterpretted J Nightingale's presciption to mean take the value of the std of the clipped image (in this case ~150000) as the background sky level. I'm not adding noise variance, just the background sky level. Okay.
Problem is, the minimum values of counts in the image are ~ -400000, so it still leaves negative numbers for the square root. K Kuijken said that the background should be of order 10-100 eps. Typical CCD gains (the ACTUAL definition of the gain) are around 7-10 e-/ADU. So I assume 100 eps, gain of 10, and exp_time of 1200 to see what happens... it adds a constant value of 1200000 counts and is the only thing that has remotely looked correct when the image is loaded by autolens. Hmm. 
Update, K Kuijken has also been referring to electron counts. So I don't need to use the gain after I convert it from ADUs to electron counts (which is what pyautolens and J Nightingale wanted all along). Then I just need to multiply 100 eps * 1200 s to get 120000 electron counts, which I add to the image and take the sqrt. I then divide again by 1200, and that's my noise map. I'll check after lunch to see if it will converge to a fit.

2/16/21 - afternoon
Tried to figure the noise map out with Benne for a while... We came up with setting the mean to the clipping_mean*10 and then subracting the sqrt of that value at the end.

2/16/21 - morning
Looking into the flux unit problem by uploading the full tile fits file to see what the values are.
Found that LinKS made the composite RGB images using HumVI (https://github.com/drphilmarshall/HumVI), but since the fits files are r, g, and i separated, I expect that they either reseparated them or showed only the raw cutouts.
If LinKS messed with the images, I'll just make my own cutouts.

2/15/21
Still struggling with the units from KiDS and autolens.
J Nightingale admitted that his "counts" means electron counts, which is not typical usage.
K Kuijken's prescription for converting from "calibrated flux units" was to multiply by *either* the gain or the gain*exp_time... which is an important distinction.
The calibrated flux units are negative in many pixels, which means they are converted to negative counts, which is not only nonsense but it leads to non-real numbers when I take the sqrt for the noise.
I'm going to leave this and focus on the environment catalogs from GAMA until I have a clear way forward.

2/12/21 - afternoon
New plan, no weight maps.
1. image to counts_image
2. sigma clip counts_image to get background_noise
3. add background_noise to counts_image to get bg_added_image
4. sqrt(bg_added_image) to get noise_map
5. convert counts_image and noise_map to e-/s
I'm exhausted. I'm close, but I'm exhausted. I'll need to pick it up again on Monday and work it out. I think the noise map valuse should be much smaller, so maybe I missed a factor.

2/12/21 - morning
I was mistaken about what I needed for the units. For the final images, I need them in e-/s. The conversion factor is [counts=e-/s * exp_time]. I need the image in counts, then I get the noise value by taking 1/sqrt(weight) (or npsqrt(data)) and then convert back to e-/s by dividing the exp_time.
I'm not sure the weight map can be used directly to get the noise map (it would basically take 1/sqrt(weight) which is the rms variance). The problem is that the weight map doesn't appear to show any signal from the galaxy and may be just the background sky noise. Maybe I can use that as the background sky noise and add it to the data before taking the sqrt(data+bg) and then convert back to e-/s.
Here is the procedure:
1. npsqrt(data) in counts to get "shot-noise" of galaxies
2. take 1/sqrt(weight) to get rms noise (background)
3. multiply by gain*exp_time to get counts # This part is the problem because the gain is 0.
4. add background noise to "shot-noise"
5. sqrt(this image)
I have done step 1. I can't load the weight data because fitsio is not working properly... again.


2/11/21 - afternoon
fitsio works on my machine, but I need Lutz to update the library so I can use it on pacer. I've been able to make cutouts on the weight coadds. The problem is now that the position doesn't correspond to the galaxy. It might be close... I can't tell properly. But the Cutout2d is supposed to be able to take the reference wcs coordinates and pixels to find things.

2/11/21 - morning
Trying to work out the problem with accessing the data from the weight map. I believe the data has been truncated or compressed (it's also a large file), and I've found little help from online searches... I'm going to try a different fits opening module (fitsio).

2/10/21 - evening
Trying to use astropy.Cutout2d to create cutouts (which should be super easy) and struggling... The center pixel indices are in the thousands? It's 101x101. Makes no sense. Totally works if I just put in (30, 54) or something.
I also tried to open the weight map to see if it was just some odd mistake from the LinKS header. I couldn't even access the data from the primary hdu. "error - buffer is too small for requested array"

2/10/21 - afternoon
Response from J Nightingale (louisville.edu account) super helpful. The weight coadd is what I need. He gave me two functions that will produce the noise map from the weight map (or inverse noise map). The difference between the two is a squared term.
I had the wrong tile for the candidate I'm looking at. Have to go to a meeting with UCLA grad.

2/10/21 - morning
Response from K. Kuijken quite helpful. I was stupid, it's actually DR4, but it should be exactly the same anyway.
Note: pixel scale is 0.2 (I calculated 0.198 so that's fine)
Exposure times: g - 900s , r - 1800s, i - 1200s
Wrote functions to convert to counts.
Combined counts header info extraction, conversion, resizing, psf generation into one function (one_ring_to_rule_them_all)
Need to include the noise map when I figure that out. Breaking for lunch.


2/9/21
Still no response from Benne. I need to figure out this noise map thing. The FITS header is useless. I *think* they are in counts because the gain factor is huge and the signal is tiny, so I think it has already been converted to counts (ADUs). Maybe I completely misunderstood that, but hey how am I supposed to make something out of nothing. The lack on information in the images is frustrating. Met with Benne, and he worked through a lot of it with me. See that log for our discussion. Emailed Konrad Kuijken about DR3.

2/5/21
Tried taking sqrt of pixel values. Did not work at all.
Emailed James Nightingale (autolens creator), who said I'd need to know units of data (e-/s, counts), exposure time to convert to counts, and sky background level if not already subtracted.

2/4/21
The noise map in tutorials shows features from the galaxies... Perhaps I need to rethink that procedure. Here's how the API for autolens describes it: 'An array describing the RMS standard deviation error in each pixel in units of electrons per second.' I think that's what I've done? What about these units e-/s? Image is also that.
Downloaded tile and catalog tables that should give each tile's FWHM of psf.
To ask Benne: Gain - electrons per second?
Psf convolution was struggling, which I believe is because I made the image too small and the psf too large. Reworked some prep. Ran into another config file problem when I tried to run the model... which had inf loglikelihood measurements. Something is completely wrong that I need to figure  out.. I'm wondering if we misinterpreted the noise map generation. Perhaps I need to add the image back onto it? Why would that be the case?
Summary:
- Have tile PSFs for g, r, i
- Model starts to run; problem appears to be in noise map - don't think I have what it needs

**** I decided on 2/4/21 that it makes more sense to update at the top... ****
1/14/21
Reviewing all the work prior to today.
Cut the samples acc. to (z_lens > 0.05) and (z_src - z_lens > 0.1)
Created directory w/in csv files labeled "latest"
Most recent data as of beginning of session:
    links_autoz_sample_latest.csv (links_autoz_sample_061520) (56 rows, 52 unique candidates)
    links_knabel_autoz_sample_latest.csv (...061520) (7 rows, 6 unique candidates)
    li_autoz_sample_latest.csv (...061520) (8 rows, 8 unique candidates)
Most recent data following end of session:
    links_sample_latest_len42.csv (42 rows, 40 unique candidates)
    links_knabel_sample_len7.csv (7 rows, 6 unique candidates)
    li_sample_len3.csv (3 unique candidates)
Notes:
    I have not determined how I will choose one of the duplicates over the other.

1/15/21
Working with final two duplicates in LinKS candidates. Simple to cut.
Most recent data following session:
	links_sample_latest_len40
	links_knabel_sample_len7
	li_sample_len3
Notes:
	I intend to combine the notebooks and visualizations and begin writing.   
    
1/15/21
Consolidated work to a master notebook. Got through the most recent selection.
Most recent data following session:
	links_sample_latest_len40
	links_knabel_sample_latest_len6
	li_sample_latest_len3
Next step to pull the visualization pieces into the master notebook.
Notes:
    Check those data against the ones saved in the latest folder earlier today.
        All candidates should be the same... hopefully I got the Lambdar stuff 
        right the first time

1/18/21
Consolidated visualization code to master notebok.
Looked at two LinKS candidates that appear to fall within the selection parameter space used by Holwerda-15... One of them passes, and I have no idea why it wasn't selected in the paper.
It could have been on an alias... All candidates, old and new, near the alias of
(1+z)/(1+z2)=1.343±0.002 (∼5007/3727) or the inversewere removed from the sample.
For this candidate, the result is 1.344, which shows that it was removed by happenstance. With a log(mass) ~ 11.2 and redshift 0.22, it falls right at the overlap between spec and mac in the Knabel-2020 paper. Interesting!
Discuss with Benne the relevant info to focus on.

1/19/21
Notebook: 011921_correlation_tests
Applying tests of correlation for output parameters from autoz and lens scores.
In master notebook, CNN prob is incorrectly merged. Change that!
Ran spearman, pearson and kendall tau tests on the scores and cnn probability output to sigma2 and R. Scores were not well-correlated at all. CNN probability output was ~0.25, but there are a couple outliers. It may be useful to bootstrap and check again.

1/20/21
Notebook: 011021_correlation_tests
Fitted linear regression to the parameters. 
Very weak correlations, but I at least have some numbers to it. 
Ran some bootstrapping tests to see about uncertainties and get more info on how outliers affect it... Not sure how to do all of that properly. Emailed Benne. I used the result of the fit and put the uncertainty as +/- the std from the bootstrapping.
Added these results to the paper.
Still need to fix CNN prob merging in master notebook and put correlation studies in the master as well.

1/21/21
Updated master notebook with the correlation studies and visualizations.
Also removed a LinKS candidate whose probability of redshift success in autoz was very low.
This forced me to redo the correlation studies, which changed the numbers slightly. No change in outcome.
Looked at the z-lens redshift outputs for Li candidates against Li photo-z, which are inconsistent for two of the three. Will need to make a decision there. I need to check the z-src as well.

1/21/21
Checking on duplicates I came upon the concerning fact that I had mixed up ELG + ELG and PG + PG. Has minor consequences, but I'm glad I caught it here.
In response to my concern about redshift matches having sigma_source>sigma_lens, Benne:
    "That is more a strength of the emission line rather than anything else. This is a flux-weighted result after all. 
So if you have a whopping Halpha line, it's going to be the primary solution, even though the continuum is a lower redshift. SO this is fine.""
To Do:
Overlay the redshift and stellar mass results over the results from Knabel-2020
Further explore primary redshift as background source
Flesh out writing sections on "what could have been" and results from Li.

1/22/21 - Morning
Notebook: 012221_autoz_samples_comparison_to_knabel2020 (copied to master notebook)
Plotted lens redshift to lens stellar mass on top of big plot from knabel2020.
    All autoz candidates are at log(m*) > 11.0 but at the lower end of the range of LinKS-Knabel2020 candidates, throughout redshift range.
Calculated PM and SIS Einstein radius estimates using AUTOZ source redshifts and comparing to the 2:1 ratio we used in Knabel-2020.
    In general, AUTOZ source redshifts gave lower estimates compared to 2:1 ratio, which have mean ~ or < 1.0 arcsec.
    It appears they are typically on the lower end of the machine learning candidate mass range and Einstein radius. Especially looking at the LinKS sample, it appears these candidates have estimated Einstein radii of ~1 arcsec or less, which fits our idea that they would have a better shot of being detected by AUTOZ. Most of these didn't make the cut based on the visual inspection scores given by Petrillo, which also makes sense if their Einstein radii are particularly small (leading to low scores).
    (mean, median, std)
    LinKS PM:  1.138 0.984 0.535
    LinKS PM 2to1:  1.241 1.171 0.414
    Average Difference:  0.102
    --
    LinKS SIS:  0.857 0.771 0.352
    LinKS SIS 2to1:  1.042 1.015 0.244
    Average Difference:  0.185
    --
    LinKS Knabel-2020 PM:  1.033 0.918 0.192
    LinKS Knabel-2020 PM 2to1:  1.248 1.199 0.317
    Average Difference:  0.214
    --
    LinKS Knabel-2020 SIS:  0.757 0.726 0.141
    LinKS Knabel-2020 SIS 2to1:  1.094 1.181 0.263
    Average Difference:  0.336
    --
    Li PM:  1.304 1.043 0.777
    Li PM 2to1:  1.26 1.257 0.528
    Average Difference:  -0.044
    --
    Li SIS:  0.98 0.819 0.475
    Li SIS 2to1:  0.965 1.023 0.258
    Average Difference:  -0.015
I want to show this difference a little better and discuss all of this in the paper. For this afternoon.

1/22/21 Afternoon
I looked at mean and median stellar masses and Einstein radius estimates. KS tests show disparity to high significance between stellar mass of LinKS AUTOZ and LinKS from Knabel-2020, and an almost identical result when compared to the GAMA spectroscopic sample.
LinKS - Mass:  [0.352 0.007] (metric pvalue)
LinKS/Spec - Mass:  [0.353 0.007]
Also looked at G544226, which would have been the overlap between GAMA spec and LinKS in Knabel2020. It is the ideal for identification by both.
Plan to rename Li candidates to BG (bright galaxies) or Li-BG for sake of distinguishing them from LinKS.
Still more to write in the paper, and I need to start on reviewing SVMs on Monday.

1/25/21
ML for classification - SVMs (SVC), K-NearestNeighbor, supervised vs unsupervised...
    What is my test/train?
    KNN : curse of dimensionality
    Cross-validation strategies: KFold, etc. https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html
Stopped for lunch at sklearn tutorial 3 - unsupervised learning
Unsupervised learning:
    K-means clustering (sparsity mitigates curse of dimensionality)
    Hierarchical - Agglomerative - bottom-up approaches
        feature agglomeration mitigates curse of dimensionality
    Transforms, Decomposition, PCA to reduce dimensionality
        ICA
Pipelines... Maximize cross-validation scores from multiple algorithms... e.g. PCA and SVC?
Quadratic Surface SVM? - SVM are best for supervised learning. Adapt to unsupervised?
    https://www.sciencedirect.com/science/article/abs/pii/S0377221719306630

1/26/21
Writing more of the paper.
Reviewing ML...
Starting to think I'll want to begin with SVM supervised, tell it what to do. See what it says about the sigma2 and R parameter space.
Then give it everything, let it decide with PCA what to determine, still keep it supervised.
Then try it k-means clustering and PCA and see if it marks off anything similar.

1/28/21
Benne wanted to scrap the ML idea for now... I'll come back to it.
We're going to focus on the lens modeling, which is more fun anway!
I've reviewed PyAutolens HowTos thru Ch2.5.
Tomorrow:
Finish 6-9, look at preprocessing
Take a look at some real images
Hopefully model a KiDS image next week.

1/29/21
Finished autolens tutorials, prepping G3575500 (1906) for modeling.
Need to figure out the psf stuff and whether I should stack the images... I think that's what I did before.

2/3/21
Had a bunch of issues trying to make autolens do anything that isn't part of the tutorials.

